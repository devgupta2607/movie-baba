{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import tensorflow\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Dataframe :\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Reading the ratings data\n",
    "        self.ratings = pd.read_csv('D:\\Dataset/ratings.csv')\n",
    "    \n",
    "        #Just taking the required columns\n",
    "        self.ratings = self.ratings[['userId', 'movieId','rating']]\n",
    "    \n",
    "        # Checking if the user has rated the same movie twice, in that case we just take max of them\n",
    "        self.ratings_df = self.ratings.groupby(['userId','movieId']).aggregate(np.max)\n",
    "        \n",
    "        #reading the movies dataset\n",
    "        self.movie_list = pd.read_csv('D:\\Dataset/movies.csv')\n",
    "        \n",
    "        # reading the tags dataset\n",
    "        self.tags = pd.read_csv('D:\\Dataset/tags.csv')\n",
    "        \n",
    "        # inspecting various genres\n",
    "        self.genres = self.movie_list['genres']\n",
    "        \n",
    "        self.genre_list = \"\"\n",
    "        for index,row in self.movie_list.iterrows():\n",
    "             self.genre_list += row.genres + \"|\"\n",
    "                \n",
    "        #split the string into a list of values\n",
    "        self.genre_list_split = self.genre_list.split('|')\n",
    "        \n",
    "        #de-duplicate values\n",
    "        self.new_list = list(set(self.genre_list_split))\n",
    "        \n",
    "        #remove the value that is blank\n",
    "        self.new_list.remove('')\n",
    "        \n",
    "        #Enriching the movies dataset by adding the various genres columns.\n",
    "        self.movies_with_genres = self.movie_list.copy()\n",
    "\n",
    "        for genre in self.new_list :\n",
    "            self.movies_with_genres[genre] = self.movies_with_genres.apply(lambda _:int(genre in _.genres), axis = 1)\n",
    "            \n",
    "        #Calculating the sparsity\n",
    "        self.no_of_users = len(self.ratings['userId'].unique())\n",
    "        self.no_of_movies = len(self.ratings['movieId'].unique())\n",
    "\n",
    "        self.sparsity = round(1.0 - len(self.ratings)/(1.0*(self.no_of_movies * self.no_of_users)),3)\n",
    "        \n",
    "        # Finding the average rating for movie and the number of ratings for each movie\n",
    "        self.avg_movie_rating = pd.DataFrame(self.ratings.groupby('movieId')['rating'].agg(['mean','count']))\n",
    "        #self.avg_movie_rating['movieId']= self.avg_movie_rating.index\n",
    "        \n",
    "        #Get the average movie rating across all movies \n",
    "        self.avg_rating_all=self.ratings['rating'].mean()\n",
    "        self.avg_rating_all\n",
    "        #set a minimum threshold for number of reviews that the movie has to have\n",
    "        self.min_reviews=30\n",
    "        self.movie_score = self.avg_movie_rating.loc[self.avg_movie_rating['count']>self.min_reviews]\n",
    "        \n",
    "        #merging ratings and movies dataframes\n",
    "        self.ratings_movies = pd.merge(self.ratings , self.movie_list, on = 'movieId')\n",
    "        \n",
    "        self.flag = False\n",
    "        \n",
    "    #create a function for weighted rating score based off count of reviews\n",
    "    def weighted_rating(self , x , m = None, C = None):\n",
    "        \n",
    "        if m is None:\n",
    "            m = self.min_reviews\n",
    "            \n",
    "        if C is None:\n",
    "            C = self.avg_rating_all\n",
    "        v = x['count']\n",
    "        R = x['mean']\n",
    "        \n",
    "        # Calculation based on the IMDB formula\n",
    "        return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "    \n",
    "    # Gives the best movies according to genre based on weighted score which is calculated using IMDB formula\n",
    "    def best_movies_by_genre(self , genre , top_n):\n",
    "        \n",
    "        #Calculating the weighted score for each movie\n",
    "        if self.flag is not True:\n",
    "            self.movie_score['weighted_score'] = self.movie_score.apply(self.weighted_rating, axis=1)\n",
    "            \n",
    "            #join movie details to movie ratings\n",
    "            \n",
    "            self.movie_score = pd.merge(self.movie_score,self.movies_with_genres,on='movieId')\n",
    "            self.flag = True\n",
    "        \n",
    "        return pd.DataFrame(self.movie_score.loc[(self.movie_score[genre]==1)].sort_values(['weighted_score'],ascending = \n",
    "                                                                False)[['title','count','mean','weighted_score']][:top_n])\n",
    "    \n",
    "    #Gets the other top 10 movies which are watched by the people who saw this particular movie\n",
    "    def get_other_movies(self , movie_name , top_n):\n",
    "        #get all users who watched a specific movie\n",
    "        df_movie_users_series = self.ratings_movies.loc[self.ratings_movies['title']==movie_name]['userId']\n",
    "        \n",
    "        #convert to a data frame\n",
    "        df_movie_users = pd.DataFrame(df_movie_users_series,columns=['userId'])\n",
    "        \n",
    "        #get a list of all other movies watched by these users\n",
    "        other_movies = pd.merge(df_movie_users,self.ratings_movies,on='userId')\n",
    "        \n",
    "        #get a list of the most commonly watched movies by these other user\n",
    "        other_users_watched = pd.DataFrame(other_movies.groupby('title')['userId'].count()).sort_values('userId',ascending = \n",
    "                                                                                                        False)\n",
    "        \n",
    "        other_users_watched['perc_who_watched'] = round(other_users_watched['userId']*100/other_users_watched['userId'][0],1)\n",
    "        \n",
    "        return other_users_watched[:top_n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>Duck Soup (1933)</td>\n",
       "      <td>280</td>\n",
       "      <td>4.217857</td>\n",
       "      <td>4.151220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Singin' in the Rain (1952)</td>\n",
       "      <td>542</td>\n",
       "      <td>4.097786</td>\n",
       "      <td>4.067969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>Dr. Horrible's Sing-Along Blog (2008)</td>\n",
       "      <td>185</td>\n",
       "      <td>4.148649</td>\n",
       "      <td>4.062224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>Stop Making Sense (1984)</td>\n",
       "      <td>119</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>4.019316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>Fiddler on the Roof (1971)</td>\n",
       "      <td>165</td>\n",
       "      <td>4.048485</td>\n",
       "      <td>3.968606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Wizard of Oz, The (1939)</td>\n",
       "      <td>1229</td>\n",
       "      <td>3.947518</td>\n",
       "      <td>3.937552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Gay Divorcee, The (1934)</td>\n",
       "      <td>57</td>\n",
       "      <td>4.149123</td>\n",
       "      <td>3.935381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>Nashville (1975)</td>\n",
       "      <td>128</td>\n",
       "      <td>4.015625</td>\n",
       "      <td>3.923279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>Top Hat (1935)</td>\n",
       "      <td>120</td>\n",
       "      <td>4.004167</td>\n",
       "      <td>3.909188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>Day at the Races, A (1937)</td>\n",
       "      <td>51</td>\n",
       "      <td>4.117647</td>\n",
       "      <td>3.899730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  count      mean  weighted_score\n",
       "824                        Duck Soup (1933)    280  4.217857        4.151220\n",
       "580              Singin' in the Rain (1952)    542  4.097786        4.067969\n",
       "3909  Dr. Horrible's Sing-Along Blog (2008)    185  4.148649        4.062224\n",
       "1777               Stop Making Sense (1984)    119  4.142857        4.019316\n",
       "2647             Fiddler on the Roof (1971)    165  4.048485        3.968606\n",
       "600                Wizard of Oz, The (1939)   1229  3.947518        3.937552\n",
       "588                Gay Divorcee, The (1934)     57  4.149123        3.935381\n",
       "1438                       Nashville (1975)    128  4.015625        3.923279\n",
       "624                          Top Hat (1935)    120  4.004167        3.909188\n",
       "3297             Day at the Races, A (1937)     51  4.117647        3.899730"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Dataframe()\n",
    "df.best_movies_by_genre('Musical',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Band of Brothers (2001)</td>\n",
       "      <td>228</td>\n",
       "      <td>4.353070</td>\n",
       "      <td>4.257280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>City of God (Cidade de Deus) (2002)</td>\n",
       "      <td>646</td>\n",
       "      <td>4.258514</td>\n",
       "      <td>4.226151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>North by Northwest (1959)</td>\n",
       "      <td>817</td>\n",
       "      <td>4.250306</td>\n",
       "      <td>4.224768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814</th>\n",
       "      <td>Dark Knight, The (2008)</td>\n",
       "      <td>1031</td>\n",
       "      <td>4.242968</td>\n",
       "      <td>4.222788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>Fight Club (1999)</td>\n",
       "      <td>2085</td>\n",
       "      <td>4.219185</td>\n",
       "      <td>4.209399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>Raiders of the Lost Ark (Indiana Jones and the...</td>\n",
       "      <td>2289</td>\n",
       "      <td>4.206204</td>\n",
       "      <td>4.197446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Seven Samurai (Shichinin no samurai) (1954)</td>\n",
       "      <td>594</td>\n",
       "      <td>4.223906</td>\n",
       "      <td>4.190510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
       "      <td>2874</td>\n",
       "      <td>4.189457</td>\n",
       "      <td>4.182637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>Matrix, The (1999)</td>\n",
       "      <td>2705</td>\n",
       "      <td>4.176340</td>\n",
       "      <td>4.169242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Princess Bride, The (1987)</td>\n",
       "      <td>1725</td>\n",
       "      <td>4.177391</td>\n",
       "      <td>4.166312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  count      mean  \\\n",
       "3230                            Band of Brothers (2001)    228  4.353070   \n",
       "2937                City of God (Cidade de Deus) (2002)    646  4.258514   \n",
       "589                           North by Northwest (1959)    817  4.250306   \n",
       "3814                            Dark Knight, The (2008)   1031  4.242968   \n",
       "1838                                  Fight Club (1999)   2085  4.219185   \n",
       "769   Raiders of the Lost Ark (Indiana Jones and the...   2289  4.206204   \n",
       "1256        Seven Samurai (Shichinin no samurai) (1954)    594  4.223906   \n",
       "199           Star Wars: Episode IV - A New Hope (1977)   2874  4.189457   \n",
       "1629                                 Matrix, The (1999)   2705  4.176340   \n",
       "768                          Princess Bride, The (1987)   1725  4.177391   \n",
       "\n",
       "      weighted_score  \n",
       "3230        4.257280  \n",
       "2937        4.226151  \n",
       "589         4.224768  \n",
       "3814        4.222788  \n",
       "1838        4.209399  \n",
       "769         4.197446  \n",
       "1256        4.190510  \n",
       "199         4.182637  \n",
       "1629        4.169242  \n",
       "768         4.166312  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.best_movies_by_genre('Action',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Wallace &amp; Gromit: A Close Shave (1995)</td>\n",
       "      <td>622</td>\n",
       "      <td>4.176045</td>\n",
       "      <td>4.146286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Wallace &amp; Gromit: The Wrong Trousers (1993)</td>\n",
       "      <td>798</td>\n",
       "      <td>4.133459</td>\n",
       "      <td>4.111568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Grand Day Out with Wallace and Gromit, A (1989)</td>\n",
       "      <td>379</td>\n",
       "      <td>4.100264</td>\n",
       "      <td>4.058382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>WALL路E (2008)</td>\n",
       "      <td>654</td>\n",
       "      <td>4.074159</td>\n",
       "      <td>4.050260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>My Neighbor Totoro (Tonari no Totoro) (1988)</td>\n",
       "      <td>262</td>\n",
       "      <td>4.104962</td>\n",
       "      <td>4.045816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>Christmas Story, A (1983)</td>\n",
       "      <td>686</td>\n",
       "      <td>4.035714</td>\n",
       "      <td>4.014495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>How to Train Your Dragon (2010)</td>\n",
       "      <td>234</td>\n",
       "      <td>4.064103</td>\n",
       "      <td>4.003326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>Up (2009)</td>\n",
       "      <td>502</td>\n",
       "      <td>4.030876</td>\n",
       "      <td>4.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>Toy Story 3 (2010)</td>\n",
       "      <td>306</td>\n",
       "      <td>4.042484</td>\n",
       "      <td>3.996661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>Laputa: Castle in the Sky (Tenk没 no shiro Rapy...</td>\n",
       "      <td>183</td>\n",
       "      <td>4.046448</td>\n",
       "      <td>3.973606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  count      mean  \\\n",
       "518              Wallace & Gromit: A Close Shave (1995)    622  4.176045   \n",
       "741         Wallace & Gromit: The Wrong Trousers (1993)    798  4.133459   \n",
       "793     Grand Day Out with Wallace and Gromit, A (1989)    379  4.100264   \n",
       "3833                                      WALL路E (2008)    654  4.074159   \n",
       "2927       My Neighbor Totoro (Tonari no Totoro) (1988)    262  4.104962   \n",
       "1753                          Christmas Story, A (1983)    686  4.035714   \n",
       "3991                    How to Train Your Dragon (2010)    234  4.064103   \n",
       "3931                                          Up (2009)    502  4.030876   \n",
       "4004                                 Toy Story 3 (2010)    306  4.042484   \n",
       "2989  Laputa: Castle in the Sky (Tenk没 no shiro Rapy...    183  4.046448   \n",
       "\n",
       "      weighted_score  \n",
       "518         4.146286  \n",
       "741         4.111568  \n",
       "793         4.058382  \n",
       "3833        4.050260  \n",
       "2927        4.045816  \n",
       "1753        4.014495  \n",
       "3991        4.003326  \n",
       "3931        4.002591  \n",
       "4004        3.996661  \n",
       "2989        3.973606  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run function to return top recommended movies by genre\n",
    "df.best_movies_by_genre('Children',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>perc_who_watched</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gone Girl (2014)</th>\n",
       "      <td>61</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matrix, The (1999)</th>\n",
       "      <td>54</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inception (2010)</th>\n",
       "      <td>53</td>\n",
       "      <td>86.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fight Club (1999)</th>\n",
       "      <td>52</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shawshank Redemption, The (1994)</th>\n",
       "      <td>52</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dark Knight, The (2008)</th>\n",
       "      <td>52</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lord of the Rings: The Fellowship of the Ring, The (2001)</th>\n",
       "      <td>51</td>\n",
       "      <td>83.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lord of the Rings: The Return of the King, The (2003)</th>\n",
       "      <td>50</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulp Fiction (1994)</th>\n",
       "      <td>48</td>\n",
       "      <td>78.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Silence of the Lambs, The (1991)</th>\n",
       "      <td>47</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    userId  perc_who_watched\n",
       "title                                                                       \n",
       "Gone Girl (2014)                                        61             100.0\n",
       "Matrix, The (1999)                                      54              88.5\n",
       "Inception (2010)                                        53              86.9\n",
       "Fight Club (1999)                                       52              85.2\n",
       "Shawshank Redemption, The (1994)                        52              85.2\n",
       "Dark Knight, The (2008)                                 52              85.2\n",
       "Lord of the Rings: The Fellowship of the Ring, ...      51              83.6\n",
       "Lord of the Rings: The Return of the King, The ...      50              82.0\n",
       "Pulp Fiction (1994)                                     48              78.7\n",
       "Silence of the Lambs, The (1991)                        47              77.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.get_other_movies('Gone Girl (2014)' , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        with open(\"D:\\Movie Recommender Chat Bot/recommendintents.json\") as file:\n",
    "            self.filedata = json.load(file)\n",
    "\n",
    "        self.words = []\n",
    "        \n",
    "        self.labels = []\n",
    "        \n",
    "        self.docs_x = []\n",
    "        \n",
    "        self.docs_y = []\n",
    "        \n",
    "        self.stemmer = LancasterStemmer()\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english')) \n",
    "\n",
    "        for intent in self.filedata['intents']:\n",
    "            \n",
    "            for pattern in intent['patterns']:\n",
    "                \n",
    "                wrds = word_tokenize(pattern)\n",
    "                wrds = [w for w in wrds if w not in self.stop_words]\n",
    "                self.words.extend(wrds)\n",
    "                self.docs_x.append(wrds)\n",
    "                self.docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "            if intent['tag'] not in self.labels:\n",
    "                self.labels.append(intent['tag'])\n",
    "\n",
    "        self.words = [self.stemmer.stem(w.lower()) for w in self.words if w != \"?\"]\n",
    "        self.words = sorted(list(set(self.words)))\n",
    "\n",
    "\n",
    "        self.labels = sorted(self.labels)\n",
    "        \n",
    "    def get_training_data(self):\n",
    "        \n",
    "        training = []\n",
    "        \n",
    "        output = []\n",
    "\n",
    "        out_empty = [0 for _ in range(len(self.labels))]\n",
    "\n",
    "        for x, doc in enumerate(self.docs_x):\n",
    "            bag = []\n",
    "\n",
    "            wrds = [self.stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "            for w in self.words:\n",
    "                if w in wrds:\n",
    "                    bag.append(1)\n",
    "                else:\n",
    "                    bag.append(0)\n",
    "\n",
    "            output_row = out_empty[:]\n",
    "            output_row[self.labels.index(self.docs_y[x])] = 1\n",
    "\n",
    "            #dataset.append((bag , output_row))\n",
    "            training.append(bag)\n",
    "            output.append(output_row)\n",
    "            \n",
    "        training = np.array(training)\n",
    "        \n",
    "        output = np.array(output)\n",
    "        \n",
    "        from sklearn.utils import shuffle\n",
    "        \n",
    "        training , output = shuffle(training , output , random_state = 0)\n",
    "        \n",
    "        return training , output\n",
    "    \n",
    "    def bag_of_words(self , s):\n",
    "        bag = [0 for _ in range(len(self.words))]\n",
    "    \n",
    "        s_words = word_tokenize(s)\n",
    "        s_words = [self.stemmer.stem(word.lower()) for word in s_words]\n",
    "        s_words = [w for w in s_words if w not in self.stop_words]\n",
    "\n",
    "        for se in s_words:\n",
    "            for i , w in enumerate(self.words):\n",
    "                if w == se:\n",
    "                    bag[i] = 1\n",
    "\n",
    "        return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training , output = data.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self , training , output):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(len(training[0])  , 8)\n",
    "        self.fc2 = nn.Linear(8 , 8)\n",
    "\n",
    "        self.fc3 = nn.Linear(8 , len(output[0]))\n",
    "\n",
    "    def forward(self , x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return F.softmax(x , dim = 1)\n",
    "\n",
    "net = Net(training , output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Train_Model:\n",
    "    \n",
    "    def __init__(self  , training , output):\n",
    "        \n",
    "        self.optimizer = optim.Adam(net.parameters() , lr = 0.001)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "        self.X = torch.tensor(training , dtype = torch.float32)\n",
    "        self.Y = torch.tensor(output , dtype = torch.float32)\n",
    "        \n",
    "    def start_training(self , net , BATCH_SIZE , EPOCHS):\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            \n",
    "            for i in range(0 , len(self.X) , BATCH_SIZE):\n",
    "                #print(i , i+BATCH_SIZE)\n",
    "                batch_X = self.X[i : i + BATCH_SIZE].view(-1 , len(self.X[0]))\n",
    "                batch_Y = self.Y[i : i + BATCH_SIZE]\n",
    "        \n",
    "                net.zero_grad()\n",
    "                outputs = net(batch_X)\n",
    "                loss = self.loss_function(outputs , batch_Y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            print(loss)\n",
    "            \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Train_Model(training , output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0893, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0891, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0890, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0888, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0886, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0884, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0882, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0879, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0875, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0873, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0871, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0859, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0856, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0850, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0847, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0844, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0841, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0838, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0831, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0827, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0810, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0799, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0793, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0779, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0771, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0763, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0745, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0735, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0724, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0712, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0699, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0685, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0670, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0655, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0638, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0620, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0602, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0582, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0562, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0542, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0522, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0503, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0484, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0466, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0449, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0433, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0418, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0404, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0391, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0379, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0368, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0357, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0346, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0337, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0328, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0319, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0301, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0293, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0285, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0276, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0268, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0260, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0252, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0243, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0235, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0227, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0219, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0211, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0203, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0195, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0188, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0180, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0172, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0165, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0151, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0119, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0114, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(9.9347e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.7948e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.6601e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.5258e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.3961e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.2673e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.1440e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.0223e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.9043e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.7822e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.6679e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.5516e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.4415e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.3331e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.2243e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.1196e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.0186e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.9170e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.8188e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.7249e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.6294e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.5367e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.4436e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.3531e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.2657e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.1776e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.0920e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.0099e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.9295e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8475e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.7665e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.6899e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.6126e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.5386e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4678e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3934e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3201e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2530e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1870e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1182e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0499e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9840e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9204e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8551e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7952e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7329e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6719e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6118e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5518e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4957e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4413e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3835e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3266e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2727e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2197e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1692e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1164e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.0637e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.0151e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.9632e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.9153e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8695e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8220e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7753e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7295e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.6814e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.6345e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.5922e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.5496e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.5067e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4652e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4228e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.3830e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.3418e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2994e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2615e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2232e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.1834e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.1455e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.1089e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.0713e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.0350e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.9975e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.9625e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.9286e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.8947e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.8595e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.8258e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.7921e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.7582e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.7256e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6942e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6642e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6333e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6012e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5694e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5397e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5094e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.4797e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.4527e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.4238e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.3934e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.3654e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.3391e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.3118e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.2842e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.2584e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.2315e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.2045e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.1791e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.1528e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.1269e-05, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1024e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.0780e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.0522e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.0283e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.0050e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9819e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9597e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9364e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9133e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.8901e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.8673e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.8464e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.8249e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.8029e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.7808e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.7597e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.7385e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.7181e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.6982e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.6777e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.6574e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.6382e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.6181e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5987e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5794e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5601e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5421e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5234e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5050e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.4863e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.4683e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.4517e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.4337e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.4158e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3982e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3807e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3635e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3469e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3309e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3140e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2974e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2812e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2653e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2497e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2344e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2183e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2029e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1877e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1728e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1580e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1430e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1283e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1136e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0989e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0851e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0712e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0571e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0430e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0295e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0157e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0018e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9887e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9758e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9626e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9498e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9373e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9244e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9112e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8981e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8858e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8740e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8621e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8497e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8373e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8253e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8138e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8026e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7908e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7792e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7681e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7567e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7454e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7345e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7236e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7124e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7014e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6910e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6802e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6700e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6596e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6489e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6387e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6285e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6184e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6080e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5983e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5890e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5789e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5691e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5597e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5501e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5404e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5312e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5220e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5124e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5033e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4944e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4853e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4765e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4675e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4583e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4499e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4417e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4326e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4240e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4158e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4073e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3989e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3910e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3829e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3745e-05, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = tr.start_training(net , 8 , 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        with torch.no_grad() :\n",
    "            results = net(torch.tensor(data.bag_of_words(inp) , dtype = torch.float32).view(1 , len(data.words)))[0]\n",
    "            results_index = np.argmax(np.array(results))\n",
    "            tag = data.labels[results_index]\n",
    "            \n",
    "            if results[results_index] > 0.6:\n",
    "                \n",
    "                if(tag == \"toprated\"):\n",
    "                    print(\"Please give me genre of the movies you want to watch :\")\n",
    "                    genr = input()\n",
    "                    \n",
    "                    if genr not in df.new_list:\n",
    "                        print(\"I don't have this genre\")\n",
    "                        continue\n",
    "                        \n",
    "                    print(\"Please tell me number of movies that i should suggest you\")\n",
    "                    top_n = int(input())\n",
    "                    print(df.best_movies_by_genre(genr , top_n))\n",
    "                    \n",
    "                if(tag == \"contentbasedrated\"):\n",
    "                    print(\"Please tell me name of your favorite movie :\")\n",
    "                    mov = input()\n",
    "                    \n",
    "                    print(\"Please tell me number of movies that i should suggest you\")\n",
    "                    top_n = int(input())\n",
    "                    print(df.get_other_movies(mov , top_n))\n",
    "                    \n",
    "                for tg in data.filedata[\"intents\"]:\n",
    "                    if tg['tag'] == tag:\n",
    "                        responses = tg['responses']\n",
    "\n",
    "                print(random.choice(responses))\n",
    "\n",
    "            else:\n",
    "                print(\"I did'nt get that , please ask another question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)\n",
      "You: hello\n",
      "Hello!\n",
      "You: what is your n ame\n",
      "I did'nt get that , please ask another question.\n",
      "You: what is your name?\n",
      "I'm MovieBaba aka Gyaniyon ka Gyani.\n",
      "You: what is your work?\n",
      "I know everything about movies and you are lucky to get recommendations from me\n",
      "You: who is your favorite teacher?\n",
      "Getanjali Bhola mam is my favorite teacher\n",
      "You: Recommend me your top rated movies\n",
      "Please give me genre of the movies you want to watch :\n",
      "10\n",
      "I don't have this genre\n",
      "You: Recommend me your top rated movies\n",
      "Please give me genre of the movies you want to watch :\n",
      "Action\n",
      "Please tell me number of movies that i should suggest you\n",
      "10\n",
      "                                                  title  count      mean  \\\n",
      "3230                            Band of Brothers (2001)    228  4.353070   \n",
      "2937                City of God (Cidade de Deus) (2002)    646  4.258514   \n",
      "589                           North by Northwest (1959)    817  4.250306   \n",
      "3814                            Dark Knight, The (2008)   1031  4.242968   \n",
      "1838                                  Fight Club (1999)   2085  4.219185   \n",
      "769   Raiders of the Lost Ark (Indiana Jones and the...   2289  4.206204   \n",
      "1256        Seven Samurai (Shichinin no samurai) (1954)    594  4.223906   \n",
      "199           Star Wars: Episode IV - A New Hope (1977)   2874  4.189457   \n",
      "1629                                 Matrix, The (1999)   2705  4.176340   \n",
      "768                          Princess Bride, The (1987)   1725  4.177391   \n",
      "\n",
      "      weighted_score  \n",
      "3230        4.257280  \n",
      "2937        4.226151  \n",
      "589         4.224768  \n",
      "3814        4.222788  \n",
      "1838        4.209399  \n",
      "769         4.197446  \n",
      "1256        4.190510  \n",
      "199         4.182637  \n",
      "1629        4.169242  \n",
      "768         4.166312  \n",
      "These are top rated movies for genere you have chosen I hope that you enjoy these movies :)\n",
      "You: predict movie that i will love\n",
      "Please tell me name of your favorite movie :\n",
      "Star Wars: Episode IV - A New Hope (1977)\n",
      "Please tell me number of movies that i should suggest you\n",
      "6\n",
      "                                                    userId  perc_who_watched\n",
      "title                                                                       \n",
      "Star Wars: Episode IV - A New Hope (1977)             2874             100.0\n",
      "Star Wars: Episode V - The Empire Strikes Back ...    2067              71.9\n",
      "Star Wars: Episode VI - Return of the Jedi (1983)     2060              71.7\n",
      "Matrix, The (1999)                                    1747              60.8\n",
      "Raiders of the Lost Ark (Indiana Jones and the ...    1745              60.7\n",
      "Pulp Fiction (1994)                                   1745              60.7\n",
      "Buy some popcorn and watch these movies you will definately love these :)\n",
      "You: goodbye it was nice talking to you\n",
      "Goodbye!\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
